{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86d42f07-7af7-49d8-a0f4-20f3fdfe8f2b",
   "metadata": {},
   "source": [
    "2023.9.13\n",
    "\n",
    "Ziheng Deng\n",
    "\n",
    "This is a jupyter notebook for reimplementing TT U-Net.\n",
    "\n",
    "The codes are based on \"SwinIR\" (https://github.com/JingyunLiang/SwinIR), and we thank for their works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39660518-3cda-4e91-8883-3abc4013f7f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim, autograd\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "from prefetch_generator import BackgroundGenerator\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import torchvision\n",
    "from math import exp\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd051269-d5df-428a-b7de-ddd80f1ccceb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The TT U-Net was designed for cardiac CT image deblurring. It takes a 2D+t image series as the input. In our paper, the input size\n",
    "is 1x48x256x256 (Batch size, frames, W, H). About 38 G GPU memory is required to train this model with default setting. An Nvidia A100\n",
    "(40G) was used in our experiment.\n",
    "The TT U-Net does not restrict the length of the video. However, we recommend to use a longer video clip. You may crop the video along \n",
    "the spatial dimension (in a patch based way) to train the model with a consumer-level GPU.\n",
    "\"\"\"\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 20\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d337e09-e7fd-4fd7-b5e2-81e490652fa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "read .mat file.\n",
    "v -- blurred image 256x256x48\n",
    "v_gt -- ground truth image 256x256x48\n",
    "\"\"\"\n",
    "class simudata(Dataset):\n",
    "    def __init__(self,simu_dir,transform=None):\n",
    "        self.simu_dir = simu_dir\n",
    "        self.transform = transform\n",
    "        self.simu = os.listdir(self.simu_dir)\n",
    "        self.simu.sort()  \n",
    "    def __len__(self):\n",
    "        return len(self.simu)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        simu_index = self.simu[index]\n",
    "        simu_path = os.path.join(self.simu_dir,simu_index)\n",
    "        \n",
    "        with h5py.File(simu_path,'r') as f:\n",
    "            data = f.get('v') \n",
    "            v=torch.tensor(np.array(data) ,dtype=float)\n",
    "            f.close()\n",
    "                \n",
    "        with h5py.File(simu_path,'r') as f:\n",
    "            data = f.get('v_gt') \n",
    "            v_gt=torch.tensor(np.array(data) ,dtype=float)\n",
    "            f.close()\n",
    "                \n",
    "        v=v.float().unsqueeze(0)/1000\n",
    "        v_gt=v_gt.float().unsqueeze(0)/1000\n",
    "      \n",
    "        return v,v_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd549b6-35ad-4b53-9396-27986f56095a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "simudataset = simudata('.../train_data')\n",
    "evaldataset = simudata('.../eval_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0def65b2-5683-4725-b508-ee4f91ea198d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataLoaderX(DataLoader):\n",
    "\n",
    "    def __iter__(self):\n",
    "        return BackgroundGenerator(super().__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739557c2-70ef-4ef9-9433-58cbb8a0650e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "set num_workers=0 if using Windows.\n",
    "\"\"\"\n",
    "simuloader = DataLoaderX(simudataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=16)\n",
    "evalloader = DataLoaderX(evaldataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e9c797-5e37-4d79-b752-a3227a585923",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class convwithactivation(nn.Module):\n",
    "    def __init__(self,in_ch,out_ch,kernel_size=3,padding=1,stride=1,dilation=1):\n",
    "        super(convwithactivation,self).__init__()\n",
    "        self.conv=nn.Conv3d(in_ch,out_ch,kernel_size,stride,padding,dilation)\n",
    "        self.lrelu=nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = self.lrelu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9aca59-1ddf-41c6-8d5a-7670893a87df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class upconvwithactivation(nn.Module):\n",
    "    def __init__(self,in_ch,out_ch,kernel_size=3,stride=2,padding=1,scale_factor=1):\n",
    "        super(upconvwithactivation,self).__init__()\n",
    "        self.scale_factor=scale_factor\n",
    "        self.conv=nn.ConvTranspose3d(in_ch,out_ch,kernel_size,stride,padding)\n",
    "        self.lrelu=nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.interpolate(x,scale_factor=self.scale_factor,mode='nearest')\n",
    "        x = self.conv(x)\n",
    "        x = self.lrelu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab09900c-4fd6-4755-9fa0-e672427dfc4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class deconvwithactivation(nn.Module):\n",
    "    def __init__(self,in_ch,out_ch,kernel_size=3,padding=1,stride=1):\n",
    "        super(deconvwithactivation,self).__init__()\n",
    "        self.conv=nn.ConvTranspose3d(in_ch,out_ch,kernel_size,stride,padding)\n",
    "        self.lrelu=nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = self.lrelu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63343260-41ff-4c22-acb1-1f5bfff8e926",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class convwithactivation2(nn.Module):\n",
    "    def __init__(self,in_ch,out_ch,kernel_size=3,padding=1,stride=1,dilation=1):\n",
    "        super(convwithactivation2,self).__init__()\n",
    "        self.conv=nn.Conv2d(in_ch,out_ch,kernel_size,stride,padding,dilation)\n",
    "        self.BN=nn.BatchNorm2d(out_ch)\n",
    "        self.lrelu=nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = self.BN(x)\n",
    "        x = self.lrelu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50009a5-4b5b-4fdb-b381-1c001227b27e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5c264f-d85f-4cc0-9f01-559b6d47524f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WindowAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64a85da-2861-4ac5-a18b-82db7f576052",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=1, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "    \n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, C = x.shape  #L=H*W*S\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        #x = x.view(B, H, W, C)\n",
    "\n",
    "        # partition windows\n",
    "        #x_windows = window_partition(x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        \n",
    "        #x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
    "        x_windows = x.view(-1, 48, C)  # B*H*W, S, C\n",
    "\n",
    "        x = self.attn(x_windows)  # B*H*W, S, C\n",
    "        x = x.view(B,L,C)\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \"                f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74a088b-ac93-46fc-a995-64634a50aded",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "\n",
    "    def __init__(self, img_size=256, patch_size=2, in_chans=1, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.conv=nn.Conv3d(in_chans,4*in_chans,kernel_size=[1,3,3],padding=[0,1,1],stride=[1,2,2])\n",
    "        \n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.conv(x)\n",
    "        B,C,S,H,W = x.shape\n",
    "        x = x.permute(0,3,4,2,1).reshape(B,-1,C)\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56d2a4a-222f-40e4-9ac7-029009a3906b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchUnEmbed(nn.Module):\n",
    "\n",
    "    def __init__(self, img_size=256, patch_size=1, in_chans=1, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.conv=nn.ConvTranspose3d(in_chans,int(in_chans/4),[1,3,3],padding=[0,1,1],stride=[1,1,1])\n",
    "        #self.conv=nn.Conv3d(in_chans,int(in_chans/4),kernel_size=[1,3,3],padding=[0,1,1],stride=[1,1,1])\n",
    "        \n",
    "    def forward(self, x, x_size):\n",
    "        B, HWS, C = x.shape\n",
    "        x = x.view(B,x_size[0],x_size[1],-1,C).permute(0,4,3,1,2) #B*C*S*H*W\n",
    "        x=F.interpolate(x,scale_factor=[1,2,2],mode='nearest')\n",
    "        x=self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817cba99-2842-40a8-b5d2-285f76be5101",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TTUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TTUNet,self).__init__()\n",
    "        cnum=24\n",
    "        ##input=batchsize*1*48*256*256\n",
    "        self.conv1=convwithactivation(1,cnum,kernel_size=[1,3,3],padding=[0,1,1],stride=[1,1,1])\n",
    "        self.conv1_2=convwithactivation(cnum,cnum,kernel_size=[1,3,3],padding=[0,1,1],stride=[1,1,1])\n",
    "        \n",
    "        self.convt1_1=convwithactivation(cnum,cnum,kernel_size=[5,1,1],padding=[2,0,0],stride=[1,1,1])\n",
    "        self.convt1_2=convwithactivation(cnum,cnum,kernel_size=[5,1,1],padding=[2,0,0],stride=[1,1,1])\n",
    "        self.convt1_3=convwithactivation(cnum,cnum,kernel_size=[5,1,1],padding=[2,0,0],stride=[1,1,1])\n",
    "        \n",
    "        ##48*256*256->48*128*128\n",
    "        self.conv2=convwithactivation(cnum,2*cnum,kernel_size=[1,3,3],padding=[0,1,1],stride=[1,2,2])\n",
    "        self.conv3=convwithactivation(2*cnum,2*cnum,kernel_size=[1,3,3],padding=[0,1,1],stride=[1,1,1])\n",
    "        self.conv3_2=convwithactivation(2*cnum,2*cnum,kernel_size=[1,3,3],padding=[0,1,1],stride=[1,1,1])\n",
    "        \n",
    "        self.embed1=PatchEmbed(in_chans=2*cnum)\n",
    "        self.trans1_1=TransformerBlock(dim=8*cnum, input_resolution=[64,64], num_heads=torch.tensor(cnum*32/16,dtype=int), window_size=1, shift_size=0,mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.05,act_layer=nn.GELU, norm_layer=nn.LayerNorm)\n",
    "        self.trans1_2=TransformerBlock(dim=8*cnum, input_resolution=[64,64], num_heads=torch.tensor(cnum*32/16,dtype=int), window_size=1, shift_size=0,mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.05,act_layer=nn.GELU, norm_layer=nn.LayerNorm)\n",
    "        self.unembed1=PatchUnEmbed(in_chans=8*cnum)\n",
    "        self.convt1=convwithactivation(2*cnum,2*cnum,kernel_size=[3,1,1],padding=[1,0,0],stride=[1,1,1])\n",
    "        \n",
    "        ##48*128*128->48*64*64\n",
    "        self.conv4=convwithactivation(2*cnum,4*cnum,kernel_size=[1,3,3],padding=[0,1,1],stride=[1,2,2])\n",
    "        self.conv5=convwithactivation(4*cnum,4*cnum,kernel_size=[1,3,3],padding=[0,1,1],stride=[1,1,1])\n",
    "        self.conv5_2=convwithactivation(4*cnum,4*cnum,kernel_size=[1,3,3],padding=[0,1,1],stride=[1,1,1])\n",
    "        \n",
    "        self.embed2=PatchEmbed(in_chans=4*cnum)\n",
    "        self.trans2_1=TransformerBlock(dim=16*cnum, input_resolution=[32,32], num_heads=torch.tensor(cnum*16/16,dtype=int), window_size=1, shift_size=0,mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.1,act_layer=nn.GELU, norm_layer=nn.LayerNorm)\n",
    "        self.trans2_2=TransformerBlock(dim=16*cnum, input_resolution=[32,32], num_heads=torch.tensor(cnum*16/16,dtype=int), window_size=1, shift_size=0,mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.1,act_layer=nn.GELU, norm_layer=nn.LayerNorm)\n",
    "        self.unembed2=PatchUnEmbed(in_chans=16*cnum)\n",
    "        self.convt2=convwithactivation(4*cnum,4*cnum,kernel_size=[3,1,1],padding=[1,0,0],stride=[1,1,1])\n",
    "        \n",
    "        ##48*64*64->48*32*32\n",
    "        self.conv6=convwithactivation(4*cnum,8*cnum,kernel_size=[1,3,3],padding=[0,1,1],stride=[1,2,2])\n",
    "        self.conv7=convwithactivation(8*cnum,8*cnum,kernel_size=[1,3,3],padding=[0,1,1],stride=[1,1,1])\n",
    "        self.conv7_2=convwithactivation(8*cnum,8*cnum,kernel_size=[1,3,3],padding=[0,1,1],stride=[1,1,1])\n",
    "        \n",
    "        self.embed3=PatchEmbed(in_chans=8*cnum)\n",
    "        self.trans3_1=TransformerBlock(dim=32*cnum, input_resolution=[16,16], num_heads=torch.tensor(cnum*32/16,dtype=int), window_size=1, shift_size=0,mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.1,act_layer=nn.GELU, norm_layer=nn.LayerNorm)\n",
    "        self.trans3_2=TransformerBlock(dim=32*cnum, input_resolution=[16,16], num_heads=torch.tensor(cnum*32/16,dtype=int), window_size=1, shift_size=0,mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.1,act_layer=nn.GELU, norm_layer=nn.LayerNorm)\n",
    "        self.unembed3=PatchUnEmbed(in_chans=32*cnum)\n",
    "        self.convt3=convwithactivation(8*cnum,8*cnum,kernel_size=[3,1,1],padding=[1,0,0],stride=[1,1,1])\n",
    "        \n",
    "        self.conv8=deconvwithactivation(8*cnum,8*cnum,kernel_size=[1,3,3],padding=[0,1,1],stride=[1,1,1])\n",
    "        \n",
    "        ##48*32*32->48*64*64\n",
    "        self.conv9=upconvwithactivation(8*cnum,4*cnum,kernel_size=[1,3,3],stride=1,padding=[0,1,1],scale_factor=[1,2,2])\n",
    "        self.conv10=deconvwithactivation(8*cnum,4*cnum,kernel_size=3,padding=1,stride=1)\n",
    "        self.conv10_2=deconvwithactivation(4*cnum,4*cnum,kernel_size=3,padding=1,stride=1)\n",
    "        ##48*64*64->48*128*128\n",
    "        self.conv11=upconvwithactivation(4*cnum,2*cnum,kernel_size=[1,3,3],stride=1,padding=[0,1,1],scale_factor=[1,2,2])\n",
    "        self.conv12=deconvwithactivation(4*cnum,2*cnum,kernel_size=3,padding=1,stride=1)\n",
    "        self.conv12_2=deconvwithactivation(2*cnum,2*cnum,kernel_size=3,padding=1,stride=1)\n",
    "        ##48*128*128->48*256*256\n",
    "        self.conv13=upconvwithactivation(2*cnum,1*cnum,kernel_size=[1,3,3],stride=1,padding=[0,1,1],scale_factor=[1,2,2])\n",
    "        ##output\n",
    "        self.conv14=deconvwithactivation(2*cnum,1*cnum,kernel_size=3,padding=1,stride=1)\n",
    "        self.conv15=deconvwithactivation(1*cnum,1*cnum,kernel_size=3,padding=1,stride=1)\n",
    "        self.conv16=nn.Conv3d(cnum,1,kernel_size=3,padding=1,stride=1)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x0=self.conv1_2(self.conv1(x)) #48*256*256\n",
    "        x0_t=self.convt1_3(self.convt1_2(self.convt1_1(x0)))\n",
    "        \n",
    "        x1=self.conv3_2(self.conv3(self.conv2(x0))) #48*128*128\n",
    "        x1_t=self.convt1(self.unembed1(self.trans1_2(self.trans1_1(self.embed1(x1))),[64,64]))+x1 #48*128*128\n",
    "        \n",
    "        x2=self.conv5_2(self.conv5(self.conv4(x1))) #48*64*64\n",
    "        x2_t=self.convt2(self.unembed2(self.trans2_2(self.trans2_1(self.embed2(x2))),[32,32]))+x2 #48*64*64\n",
    "        \n",
    "        x3=self.conv7_2(self.conv7(self.conv6(x2))) #48*32*32\n",
    "        x3_t=self.convt3(self.unembed3(self.trans3_2(self.trans3_1(self.embed3(x3))),[16,16]))+x3 #48*32*32\n",
    "        \n",
    "        x4=self.conv9(self.conv8(x3_t)) #48*64*64\n",
    "        x5=self.conv11(self.conv10_2(self.conv10(torch.cat([x4,x2_t],dim=1))))#48*128*128\n",
    "        x6=self.conv13(self.conv12_2(self.conv12(torch.cat([x5,x1_t],dim=1))))#48*256*256\n",
    "        \n",
    "        y=self.conv16(self.conv15(self.conv14(torch.cat([x6,x0_t],dim=1))))+x\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33e18b4-2bce-4d86-8189-11cebf0908da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A 2D PatchGAN is adopted.\n",
    "\"\"\"\n",
    "class Dis(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Dis,self).__init__()\n",
    "        ##input=batchsize*1*256*256\n",
    "        cnum=24\n",
    "        self.conv1=convwithactivation2(1,cnum,kernel_size=5,padding=2,stride=2)\n",
    "        self.conv2=convwithactivation2(cnum,cnum*2,kernel_size=5,padding=2,stride=2)\n",
    "        self.conv3=convwithactivation2(cnum*2,cnum*4,kernel_size=5,padding=2,stride=2)\n",
    "        self.conv4=convwithactivation2(cnum*4,cnum*8,kernel_size=4,padding=1,stride=1)\n",
    "        self.conv5=nn.Conv2d(cnum*8,1,kernel_size=4,stride=1,padding=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        y=self.conv1(x)\n",
    "        y=self.conv2(y)\n",
    "        y=self.conv3(y)\n",
    "        y=self.conv4(y)\n",
    "        y=self.conv5(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63639ca0-e85a-4c13-a5d5-c8e182a45772",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For WGAN_GP loss.\n",
    "\"\"\"\n",
    "def calc_gradient_penalty(netD, real_data, fake_data,device):\n",
    "        batch_size = real_data.size(0)\n",
    "        alpha = torch.rand(batch_size, 1, 1, 1)\n",
    "        alpha = alpha.expand_as(real_data)\n",
    "\n",
    "        alpha = alpha.to(device)\n",
    "\n",
    "        interpolates = alpha * real_data + (1 - alpha) * fake_data\n",
    "        interpolates = interpolates.requires_grad_().clone()\n",
    "\n",
    "        disc_interpolates = netD(interpolates)\n",
    "        grad_outputs = torch.ones(disc_interpolates.size())\n",
    "\n",
    "        grad_outputs = grad_outputs.to(device)\n",
    "\n",
    "        gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                                  grad_outputs=grad_outputs, create_graph=True,\n",
    "                                  retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "        gradients = gradients.view(batch_size, -1)\n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "        return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c91eb2-1c78-4560-a218-faf4d5e8a1b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model=TTUNet().to(DEVICE)\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.0001,betas=(0.9, 0.999))\n",
    "dis=Dis().to(DEVICE)\n",
    "optimizer_dis=torch.optim.Adam(dis.parameters(),lr=0.0001,betas=(0.9, 0.999))\n",
    "loss_fn = torch.nn.L1Loss()\n",
    "#writer=SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f9a1bc-6329-4c9c-9ece-2a5ac3b880d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train1(model,dis,device,trainloader,optimizer,optimizer_dis,epoch):\n",
    "    start=time.time()\n",
    "\n",
    "    loss_l1_sum=0\n",
    "    loss_wgan_d_sum=0\n",
    "    for batch_idx,(v,v_gt) in enumerate(trainloader):\n",
    "        v=v.to(device)\n",
    "        v_gt=v_gt.to(device)\n",
    "        \n",
    "        v_refine=model(v)\n",
    "        \n",
    "        # D part\n",
    "        ## wgan d loss\n",
    "        pred_real=dis(v_gt.view(-1,1,256,256))\n",
    "        pred_fake=dis(v_refine.detach().view(-1,1,256,256))\n",
    "        loss_wgan_d=torch.mean(pred_fake-pred_real)\n",
    "        ## wgan gp loss\n",
    "        loss_wgan_gp=calc_gradient_penalty(dis,v_gt.view(-1,1,256,256),v_refine.detach().view(-1,1,256,256),device)*10\n",
    "        \n",
    "        # G part\n",
    "        ## l1 loss\n",
    "        loss_l1=loss_fn(v_refine,v_gt)\n",
    "        \n",
    "        # update D\n",
    "        optimizer_dis.zero_grad()\n",
    "        loss_D=loss_wgan_d+loss_wgan_gp\n",
    "        loss_D.backward()\n",
    "        optimizer_dis.step()\n",
    "        \n",
    "        # update G\n",
    "        ## wgan g loss\n",
    "        pred_fake2=dis(v_refine.view(-1,1,256,256))\n",
    "        loss_wgan_g=-torch.mean(pred_fake2)\n",
    "        optimizer.zero_grad()\n",
    "        loss_G=loss_l1+loss_wgan_g*0.001\n",
    "        loss_G.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_l1_sum += loss_l1.cpu().item()\n",
    "        loss_wgan_d_sum += loss_wgan_d.cpu().item()\n",
    "        if(batch_idx+1)%200==0:\n",
    "\n",
    "            print('Train Epoch: %d, loss_sim %.4f, loss_wgan_d %.4f, time %.1f sec' % (epoch,loss_l1_sum,loss_wgan_d_sum*100,time.time()-start))\n",
    "            loss_l1_sum=0\n",
    "            loss_wgan_d_sum=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e87471-ed81-4dcc-ba82-4f8dce522dc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval1(model,device,trainloader,optimizer,epoch):\n",
    "    start=time.time()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_sim_sum=0\n",
    "        for batch_idx,(v,v_gt) in enumerate(trainloader):\n",
    "            v=v.to(device)\n",
    "            v_gt=v_gt.to(device)\n",
    "        \n",
    "            v_refine=model(v)\n",
    "        \n",
    "            loss_sim=loss_fn(v_refine,v_gt)\n",
    "\n",
    "            loss_sim_sum += loss_sim.cpu().item()\n",
    "        \n",
    "        print('eval Epoch: %d, loss_sim %.4f' % (epoch,loss_sim_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94f2336-768f-4f6f-9a52-9449248efc62",
   "metadata": {},
   "source": [
    "Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc3432e-f35a-42e8-a936-4dc398e8fb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrate=0.0001\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=lrate,betas=(0.9, 0.999))\n",
    "optimizer_dis=torch.optim.Adam(dis.parameters(),lr=lrate,betas=(0.9, 0.999))\n",
    "for epoch in range(1,20):\n",
    "    train1(model,dis,DEVICE,simuloader,optimizer,optimizer_dis,epoch)\n",
    "    torch.save(model.state_dict(),'.../model_%s.pth' % epoch)\n",
    "    torch.save(dis.state_dict(),'.../dis_%s.pth' % epoch)\n",
    "    if epoch%5==0:\n",
    "        lrate=lrate*0.5\n",
    "        for params in optimizer.param_groups:             \n",
    "            params['lr'] = lrate \n",
    "        for params in optimizer_dis.param_groups:             \n",
    "            params['lr'] = lrate \n",
    "    eval1(model,DEVICE,evalloader,optimizer,epoch)\n",
    "    torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a440dc-db2c-423c-a8f8-afbe235b4c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa66ba47-bc43-448a-9b7e-763a7c3a289b",
   "metadata": {},
   "source": [
    "Inference process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56f0a7a-dc5d-4ab1-93ec-870fb100be8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model=TTUNet().to(DEVICE)\n",
    "PATH1='.../pretrained_model.pth'\n",
    "model.load_state_dict(torch.load(PATH1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d917d72-2441-433d-a734-e11610f3adf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "simudataset = simudata('.../eval_data')\n",
    "\n",
    "simuloader = DataLoaderX(simudataset,batch_size=1,shuffle=True,num_workers=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    device=DEVICE\n",
    "    model.eval()\n",
    "    for batch_idx,(v,v_gt) in enumerate(simuloader):\n",
    "        v=v.to(device)\n",
    "        v_gt=v_gt.to(device)\n",
    "        v_refine=model(v) \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cb635f-0a17-473e-87de-6b597c1f68fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(2,3,1)\n",
    "frame=12\n",
    "show1=v[0][0][frame].detach().cpu()\n",
    "plt.imshow(show1,cmap='gray',vmin=-0.3,vmax=0.5)\n",
    "plt.subplot(2,3,2)\n",
    "show2=v_refine[0][0][frame].detach().cpu()\n",
    "plt.imshow(show2,cmap='gray',vmin=-0.3,vmax=0.5)\n",
    "plt.subplot(2,3,3)\n",
    "show3=v_gt[0][0][frame].detach().cpu()\n",
    "plt.imshow(show3,cmap='gray',vmin=-0.3,vmax=0.5)\n",
    "plt.subplot(2,3,4)\n",
    "plt.imshow(show1-show3,cmap='gray',vmin=-0.2,vmax=0.2)\n",
    "plt.subplot(2,3,5)\n",
    "plt.imshow(show2-show3,cmap='gray',vmin=-0.2,vmax=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464a751d-da23-4a47-aa72-11d394e9b150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaee450a-a455-4d5e-9ffa-af377a17ebb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0300df6-1812-46a4-8809-b264b969119c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
